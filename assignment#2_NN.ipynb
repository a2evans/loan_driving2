{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "import sqlite3\n",
    "import tensorflow as tf\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/noellin/Jupyter_Notebooks/loan_driving2\n"
     ]
    }
   ],
   "source": [
    "curr_path = os.path.realpath('')\n",
    "print(curr_path)\n",
    "conn = sqlite3.connect(curr_path+\"/lending-club-loan-data/database.sqlite\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"lending-club-loan-data/database.sqlite\")\n",
    "df = pd.read_sql_query(\"select loan_amnt,id,loan_status,sub_grade from loan where loan_status = 'Charged Off';\", conn)\n",
    "#print(df)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000.0\n",
      "45248\n"
     ]
    }
   ],
   "source": [
    "ddict = df.to_dict()\n",
    "print(ddict['loan_amnt'][7])\n",
    "ddict['sub_grade'][7]\n",
    "g = zip(ddict['loan_amnt'].values(),ddict['sub_grade'].values(),ddict['id'].values(),ddict['loan_status'].values())\n",
    "print(len(list(g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "852525\n",
      "[[1, 13.99, 8, 1, 1, 9.6999999999999993]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# create our data set\n",
    "conn = sqlite3.connect(curr_path+\"/lending-club-loan-data/database.sqlite\")\n",
    "# df = pd.read_sql_query(\"select loan_amnt,loan_status,sub_grade from loan where loan_status !='Current' AND loan_status !='Late (31-120 days)' AND loan_status != 'Late (16-30 days)';\", conn)\n",
    "#df_tf = pd.read_sql_query(\"select loan_amnt, term, int_rate, annual_inc, dti, home_ownership from loan where loan_status =='Fully Paid' OR loan_status == 'Charged Off' OR loan_status == 'Default'\", conn)\n",
    "\n",
    "###@@@@@ \n",
    "#         NORMALIZE and add loan_amnt and annual_inc back later \n",
    "###@@@@\n",
    "df_tf = pd.read_sql_query(\"select loan_status, term, int_rate, emp_length, home_ownership, verification_status, dti  from loan where emp_length != 'n/a' OR loan_status =='Fully Paid' OR loan_status == 'Charged Off' OR loan_status == 'Default'\", conn)\n",
    "# df_desc = pd.read_sql_query(\"select loan_amnt, loan_status, sub_grade from loan where desc != 'NULL' AND loan_status !='Current' AND loan_status !='Late (31-120 days)' AND loan_status != 'Late (16-30 days)'\", conn)\n",
    "print (len(df_tf))\n",
    "\n",
    "data = []\n",
    "results = []\n",
    "for i in range(10000):  # data set of 10000 for faster run time. Replace with range(len(df_tf)) later\n",
    "    # one hot encoding for loan status. Or 0, 1, 2\n",
    "    # loan_status = ['1', '0', '0'] if df_tf.iloc[i][0] == 'Fully Paid' else ['0', '1', '0'] if df_tf.iloc[i][0] == 'Default' else ['0', '0', '1']\n",
    "    loan_status = 0 if df_tf.iloc[i][0] == 'Fully Paid' else 1 # bad if not fully paid\n",
    "    \n",
    "    # 0 if term is 36 months, 1 if 60 months\n",
    "    term = 0 if df_tf.iloc[i][1] == ' 36 months' else 1\n",
    "    \n",
    "    # change interest rate from str to float\n",
    "    int_rate = float(df_tf.iloc[i][2].replace('%', ''))\n",
    "    \n",
    "    # get employment length\n",
    "    txt = ''.join([c for c in df_tf.iloc[i][3].lower() if not c in set(string.punctuation)])\n",
    "    emp_length_list = ([int(s) for s in txt.split() if s.isdigit()])\n",
    "    emp_length = 0 if emp_length_list == [] else emp_length_list[0]\n",
    "\n",
    "    # 0 if renting, 1 if owner\n",
    "    ownership = 0 if df_tf.iloc[i][4] == 'RENT' else 1\n",
    "    \n",
    "    # verification status, 0 for not verified, 1 for source verified and 2 for verified\n",
    "    #verification = 0 if df_tf.iloc[i][5] == 'Not Verified' else 1 if df_tf.iloc[i][5] == 'Source Verified' else 2\n",
    "    verification = 0 if df_tf.iloc[i][5] == 'Not Verified' else 1\n",
    "\n",
    "    # append in feature vector\n",
    "    data.append([term, int_rate, emp_length, ownership, verification, df_tf.iloc[i][6]]) #df_tf.iloc[i][7], df_tf.iloc[i][8]])\n",
    "                 \n",
    "    # add loan status to results \n",
    "    results.append(loan_status)\n",
    "\n",
    "print (data[-1:])\n",
    "print (results[-1:])\n",
    "X_train = data[:int(len(data)/2)]\n",
    "y_train = results[:int(len(results)/2)]\n",
    "X_val = data[int(len(data)/2): int(len(data)*0.75)]\n",
    "y_val = results[int(len(results)/2): int(len(results)*0.75)]\n",
    "X_test = data[int(len(data)*.75):]\n",
    "y_test = results[int(len(results)*.75):]\n",
    "\n",
    "conn.close()\n",
    "# feature vector - predictor will predict (\"charged off/default status\") vs. \n",
    "# [1(maybe check this),'sub_grade','loan_amnt','emp_length','home_ownership','annual_inc','verification_status','dti'(debt-to-income),'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "label_keys is not supported for n_classes=2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-19d122e533cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_train_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeature_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_real_valued_columns_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdnn_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDNNClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# dnn_clf = tf.contrib.learn.SKCompat(dnn_clf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdnn_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hidden_units, feature_columns, model_dir, n_classes, weight_column_name, optimizer, activation_fn, dropout, gradient_clip_norm, enable_centered_bias, config, feature_engineering_fn, embedding_lr_multipliers, input_layer_min_slice_size, label_keys)\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \u001b[0mweight_column_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_column_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                     \u001b[0menable_centered_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable_centered_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m                     label_keys=label_keys),\n\u001b[0m\u001b[1;32m    338\u001b[0m             \u001b[0;34m\"hidden_units\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"feature_columns\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py\u001b[0m in \u001b[0;36mmulti_class_head\u001b[0;34m(n_classes, label_name, weight_column_name, enable_centered_bias, head_name, thresholds, metric_class_ids, loss_fn, label_keys)\u001b[0m\n\u001b[1;32m    311\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"metric_class_ids invalid for n_classes==2.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabel_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label_keys is not supported for n_classes=2.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m     return _BinaryLogisticHead(\n\u001b[1;32m    315\u001b[0m         \u001b[0mlabel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: label_keys is not supported for n_classes=2."
     ]
    }
   ],
   "source": [
    "X_train_arr = np.array(X_train)\n",
    "y_train_arr = np.array(y_train)\n",
    "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(X_train_arr)\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[30,15], n_classes=2,feature_columns=feature_cols, label_keys=['0','1','2'])\n",
    "# dnn_clf = tf.contrib.learn.SKCompat(dnn_clf) \n",
    "dnn_clf.fit(X_train_arr, y_train_arr, batch_size=10, steps=5000)\n",
    "# dnn_clf.fit(X_train_arr, y_train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_val_arr = np.array(X_val)\n",
    "y_val_arr = np.array(y_val)\n",
    "y_pred = dnn_clf.predict_classes(X_val_arr, as_iterable=False)\n",
    "#print (X_val_arr)\n",
    "#print (X_val[0])\n",
    "#print(y_pred)\n",
    "print (accuracy_score(y_val_arr, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5f5a068a02dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_train_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_train_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numpy' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_arr = numpy.array(X_train)\n",
    "y_train_arr = numpy.array(y_train)\n",
    "\n",
    "print (y_train_arr[:5])\n",
    "print (y_train[:5])\n",
    "print (X_train_arr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for testing library purposes\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "print (cancer['data'])\n",
    "print (cancer['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
