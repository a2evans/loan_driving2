{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import scipy\n",
    "import sqlite3\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "curr_path = os.path.realpath('')\n",
    "print(1)\n",
    "conn = sqlite3.connect(curr_path+\"/lending-club-loan-data/database.sqlite\")\n",
    "print(2)\n",
    "# very important, select a random sample of the data for our training set & validation sets \n",
    "# \n",
    "df = pd.read_sql_query(\"select * from loan where id in(select id from loan order by random() limit 200000);\", conn)\n",
    "print(3)\n",
    "df_default = pd.read_sql_query( \"select * from loan where loan_status='Charged Off' OR loan_status='Default' OR loan_status='Late (31-120 days)' OR loan_status='Late (16-30 days)' OR loan_status='Does not meet the credit policy. Status:Charged Off' AND id in(select id from loan order by random());\", conn)\n",
    "print(4)\n",
    "df_fully_paid = pd.read_sql_query(\"select * from loan where loan_status='Issued' OR loan_status='Fully Paid' OR loan_status='In Grace Period' OR loan_status='Does not meet the credit policy. Status:Fully Paid' AND id in(select id from loan order by random());\" , conn)\n",
    "print(5)\n",
    "# df_default = pd.read_sql_query(\"select\",conn)\n",
    "conn.close()\n",
    "# df = pd.read_sql_query(\"select * from loan where id in(select id from loan order by random() limit 200000);\", conn)\n",
    "# df_validation = pd.read_sql_query(\"select * from loan where id in(selec id from loan order by random() limit 100000);\", conn)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data sets into training and validation\n",
    "# shuffle that dataframe frac = 1 means return all elements sshuffled not a preportion\n",
    "# grab a 50,000 test set\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "# write the 50,000 test set to a csv file\n",
    "df_test = pd.read_csv('X_test_set.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grab a random set of 25,000 \"default\" examples\n",
    "df_default_pick = df_default.sample(n = 50000).reset_index(drop=True)\n",
    "# grab a random set of 25,000 \"fully paid\" examples\n",
    "df_fully_paid_pick = df_fully_paid.sample(n = 50000).reset_index(drop=True)\n",
    "\n",
    "df_def_1 = df_default_pick.iloc[:int(len(df_default_pick)/2), :]\n",
    "df_def_2 = df_default_pick.iloc[int(len(df_default_pick)/2):, :]\n",
    "df_fp_1 = df_fully_paid_pick.iloc[:int(len(df_fully_paid_pick)/2), :]\n",
    "df_fp_2 = df_fully_paid_pick.iloc[int(len(df_fully_paid_pick)/2):, :]\n",
    "# append the training sets together first 25,000 defaults, next 25,000 non-defaults\n",
    "data_train = df_def_1.append(df_fp_1)\n",
    "data_validation = df_def_2.append(df_fp_2)\n",
    "# print(len(df_default))\n",
    "# print(len(df_fully_paid))\n",
    "# print(len(data_train))\n",
    "# print(data_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set up \n",
    "\n",
    "sub_g_labels = ['A1','A2','A3','A4','A5','B1','B2','B3','B4','B5','C1','C2','C3','C4','C5','D1','D2','D3'\\\n",
    "            ,'D4','D5','E1','E2','E3','E4','E5','F1','F2','F3','F4','F5','G1','G2','G3','G4','G5']\n",
    "\n",
    "# make sub_grade \n",
    "# give sub grade numeric value.\n",
    "grade_v = [(1.0/35 * i) for i in range(1, 36)]\n",
    "\n",
    "# dictionary to make sub_grade val to sub_grade string\n",
    "# sub_grade_dict = dict(zip(sub_g_labels, grade_v))\n",
    "sub_grade_dict = dict(zip(sub_g_labels, range(1, 36)))\n",
    "\n",
    "# print(sub_grade_dict)\n",
    "# print(df.axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterTuples[0(index)]\n",
    "ax = ['Index','index','id','member_id','loan_amnt','funded_amnt','funded_amnt_inv','term','int_rate','installment','grade','sub_grade'\\\n",
    "      ,'emp_title','emp_length','home_ownership','annual_inc','verification_status','issue_d','loan_status','pymnt_plan','url','desc','purpose','title','zip_code','addr_state','dti'\\\n",
    "      ,'delinq_2yrs','earliest_cr_line','inq_last_6mths','mths_since_last_delinq','mths_since_last_record','open_acc','pub_rec'\\\n",
    "      ,'revol_bal','revol_util','total_acc','initial_list_status','out_prncp','out_prncp_inv','total_pymnt','total_pymnt_inv'\\\n",
    "      ,'total_rec_prncp','total_rec_int','total_rec_late_fee','recoveries','collection_recovery_fee','last_pymnt_d','last_pymnt_amnt'\\\n",
    "      ,'next_pymnt_d','last_credit_pull_d','collections_12_mths_ex_med','mths_since_last_major_derog','policy_code','application_type'\\\n",
    "      ,'annual_inc_joint','dti_joint','verification_status_joint','acc_now_delinq','tot_coll_amt','tot_cur_bal','open_acc_6m','open_il_6m'\\\n",
    "      ,'open_il_12m','open_il_24m','mths_since_rcnt_il','total_bal_il','il_util','open_rv_12m','open_rv_24m','max_bal_bc'\\\n",
    "      ,'all_util','total_rev_hi_lim','inq_fi','total_cu_tl','inq_last_12m']\n",
    "\n",
    "index_dict = dict(zip(ax, range(len(ax))))\n",
    "# 11th spot in the pandas df if read in all columns\n",
    "sub_grade = 11\n",
    "loan_amnt = 4\n",
    "loan_status = 18\n",
    "\n",
    "def feature(datum):\n",
    "    # create our simplistic featur vector\n",
    "    #print(datum)\n",
    "    s_grade = datum[index_dict['sub_grade']]\n",
    "    # print(datum[sub_grade])\n",
    "    return [sub_grade_dict[datum[index_dict['sub_grade']]], datum[index_dict['loan_amnt']]]\n",
    "\n",
    "\n",
    "default = 0\n",
    "paid = 0\n",
    "def prediction_of_y(datum):\n",
    "    if ((datum[index_dict['loan_status']] == 'Charged Off') or (datum[index_dict['loan_status']] == 'Default') or (datum[index_dict['loan_status']] == 'Late (31-120 days)' ) or (datum[index_dict['loan_status']] == 'Late (16-30 days)') or (datum[index_dict['loan_status']] =='Does not meet the credit policy. Status:Charged Off')):\n",
    "        global default \n",
    "        default += 1\n",
    "        return True\n",
    "    else:\n",
    "        global paid \n",
    "        paid += 1\n",
    "        return False\n",
    "\n",
    "# X_train = [feature(d) for d in data_train]\n",
    "X_train = []\n",
    "pp = True\n",
    "for row in data_train.itertuples():\n",
    "    if pp: \n",
    "        # print(row)\n",
    "        pp = False\n",
    "    X_train.append(feature(row))\n",
    "    \n",
    "# y_train = [prediction_of_y(d) for d in data_train]\n",
    "y_train = []\n",
    "for row in data_train.itertuples():\n",
    "    y_train.append(prediction_of_y(row))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "dict_keys(['Charged Off', 'Default', 'Late (31-120 days)', 'Late (16-30 days)', 'Does not meet the credit policy. Status:Charged Off'])\n",
      "25000\n",
      "25000\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "l_status_dict = defaultdict(int)\n",
    "b = True\n",
    "for row in df_default.itertuples():\n",
    "    if b:\n",
    "        print(len(row))\n",
    "        # for i in range(len(row)):\n",
    "            # print(row[74])\n",
    "            #print(str(i)+' row[i] is:  '+str()\n",
    "            #print(str(i)+'  ax[i] is:  '+str(ax[i]))\n",
    "        # print(len(row))\n",
    "        b = False\n",
    "    # print(row[3])\n",
    "    if row[loan_status] not in l_status_dict:\n",
    "        l_status_dict[row[loan_status]] += 1\n",
    "        \n",
    "print(l_status_dict.keys())\n",
    "print(default)\n",
    "print(paid)\n",
    "print(len(ax))\n",
    "# print(df.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a = list(zip(X_train[:50],y_train[:50]))\n",
    "# print(a)\n",
    "# pipline to scale the feature vectors and run the svm\n",
    "rbf_svm_clf = Pipeline((\n",
    "        ('scaler', StandardScaler()),\n",
    "        (\"rbf_svc\", SVC(kernel='rbf', C=1))\n",
    "    ))\n",
    "\n",
    "linear_svm_clf = Pipeline((\n",
    "        ('scaler',StandardScaler()),\n",
    "        ('linear_svc', LinearSVC(C=1))\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clf = svm.SVC(C=1, kernel='rbf')\n",
    "# clf.fit(scaler.transform(X_train), y_train)\n",
    "rbf_svm_clf.fit(X_train, y_train)\n",
    "train_predictions = rbf_svm_clf.predict(X_train)\n",
    "\n",
    "# test_predictions = clf.predict(X_test)\n",
    "# print(train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62288\n"
     ]
    }
   ],
   "source": [
    "# rbf_svm_clf_C1000.fit(X_train, y_train)\n",
    "# train_predictions_C1000 = rbf_svm_clf_C1000.predict(X_train)\n",
    "linear_svm_clf.fit(X_train, y_train)\n",
    "train_lin_predict = linear_svm_clf.predict(X_train)\n",
    "print(find_results(train_lin_predict,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6273\n"
     ]
    }
   ],
   "source": [
    "# what % acc did we get?\n",
    "\n",
    "def find_results(tp,y_t):\n",
    "    \n",
    "    results = list(zip(tp,y_t))\n",
    "    correct = 0\n",
    "\n",
    "    for i in range(len(results)):\n",
    "        if results[i][0] == results[i][1]:\n",
    "            correct +=1\n",
    "    \n",
    "    return(correct/float(len(results)))\n",
    "\n",
    "print(find_results(train_predictions, y_train))\n",
    "# print(find_results(train_predictions_C1000, y_train))\n",
    "# print(find_results(train_predictions_C001, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "31284\n",
      "0.62568\n"
     ]
    }
   ],
   "source": [
    "# prep the SVM to run on the validation set\n",
    "X_valid = []\n",
    "for row in data_validation.itertuples():\n",
    "    X_valid.append(feature(row))\n",
    "    \n",
    "y_valid = []\n",
    "for row in data_validation.itertuples():\n",
    "    y_valid.append(prediction_of_y(row))\n",
    "\n",
    "    \n",
    "#find the acc of the validation set\n",
    "valid_predictions = rbf_svm_clf.predict(X_valid)\n",
    "\n",
    "results_valid = list(zip(valid_predictions, y_valid))\n",
    "correct_valid = 0\n",
    "flag = True\n",
    "for i in range(len(results_valid)):\n",
    "    if flag:\n",
    "        print(results_valid[i][0])\n",
    "        print(results_valid[i][1])\n",
    "        flag = False\n",
    "    if results_valid[i][0] == results_valid[i][1]:\n",
    "        correct_valid +=1\n",
    "\n",
    "print(correct_valid)\n",
    "print(correct_valid/float(len(results_valid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
